{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6wWqEvIdd46"
      },
      "outputs": [],
      "source": [
        "#Author: Pulkit Hooda\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, warnings, random\n",
        "import numpy as np\n",
        "import torch, transformers, datasets, pyarrow\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "\n",
        "#environ setup\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#set seed fpr reprod\n",
        "SEED = 42\n",
        "def set_seed(seed=SEED):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "set_seed(SEED)\n",
        "\n",
        "#paths\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/data/processed_samples\")\n",
        "OUT_DIR = Path(\"/content/outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#device picker\n",
        "def pick_device():\n",
        "  if torch.cuda.is_available(): return \"cuda\"\n",
        "  if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available(): return \"mps\"\n",
        "  return \"cpu\"\n",
        "\n",
        "device = pick_device()\n",
        "use_cuda = device == \"cuda\"\n",
        "use_mps = device == \"mps\"\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "xhV184WYdhf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(DATA_DIR / \"train_filtered.tsv\", sep=\"\\t\")\n",
        "val_df = pd.read_csv(DATA_DIR / \"val_filtered.tsv\", sep=\"\\t\")\n",
        "test_df = pd.read_csv(DATA_DIR / \"test_filtered.tsv\", sep=\"\\t\")\n",
        "\n",
        "print(\"Number of rows in train_df:\", len(train_df))\n",
        "print(\"Number of rows in val_df:\", len(val_df))\n",
        "print(\"Number of rows in test_df:\", len(test_df))"
      ],
      "metadata": {
        "id": "su5ttmVUdllB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_multimodal_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "  df = df.copy()\n",
        "  df = df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
        "\n",
        "  #confirm text col\n",
        "  if 'clean_title' in df.columns:\n",
        "    df['text'] = df['clean_title'].astype(str)\n",
        "  elif 'title' in df.columns:\n",
        "    df['text'] = df['title'].astype(str)\n",
        "  else:\n",
        "    raise ValueError(f\"No text column found in: {df.columns.tolist()}\")\n",
        "\n",
        "  #confirm label col\n",
        "  if '2_way_label' in df.columns:\n",
        "    df['2_way_label'] = pd.to_numeric(df['2_way_label'], errors='coerce')\n",
        "\n",
        "  #comfirm image path is present\n",
        "  if 'local_image_path' not in df.columns:\n",
        "    raise ValueError(\"Missing 'local_image_path' column for images\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "FaIdKHUido3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validate all the df\n",
        "train_df = validate_multimodal_df(train_df)\n",
        "val_df = validate_multimodal_df(val_df)\n",
        "test_df = validate_multimodal_df(test_df)"
      ],
      "metadata": {
        "id": "0ZjXJJ3pdsJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_df.head())"
      ],
      "metadata": {
        "id": "PBvmLoY_8fH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df only has relevant cols\n",
        "TEXT_COL, LABEL_COL, IMAGE_COL = \"text\", \"2_way_label\", \"local_image_path\"\n",
        "\n",
        "for df in (train_df, val_df, test_df):\n",
        "  df.dropna(subset=[TEXT_COL, LABEL_COL, IMAGE_COL], inplace=True)\n",
        "  df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
        "\n",
        "print(\"Label counts (train):\", train_df[LABEL_COL].value_counts())\n",
        "print(\"Label counts (val):\", val_df[LABEL_COL].value_counts())\n",
        "print(\"Label counts (test):\", test_df[LABEL_COL].value_counts())"
      ],
      "metadata": {
        "id": "9WoGVfG9dufq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, CLIPProcessor\n",
        "\n",
        "#custom dataset class\n",
        "class MultimodalDataset(Dataset):\n",
        "  def __init__(self, df, text_tokenizer, image_processor, max_length=128, drive_path=\"/content/drive/MyDrive/\"):\n",
        "    self.df = df.reset_index(drop=True)\n",
        "    self.text_tokenizer = text_tokenizer\n",
        "    self.image_processor = image_processor\n",
        "    self.max_length = max_length\n",
        "    self.drive_path = drive_path\n",
        "    self.data = self._load_data()\n",
        "\n",
        "  #load and process data\n",
        "  def _load_data(self):\n",
        "    processed_data = []\n",
        "    for idx in range(len(self.df)):\n",
        "      row = self.df.iloc[idx]\n",
        "\n",
        "      #process text\n",
        "      text = str(row[TEXT_COL])\n",
        "      #encode\n",
        "      text_encoding = self.text_tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=self.max_length,\n",
        "        return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "      #debug\n",
        "      if idx == 1:\n",
        "        print(f\"Text encoding shape for index {idx}: {text_encoding['input_ids'].shape}\")\n",
        "\n",
        "      #process image\n",
        "      try:\n",
        "        #full image path\n",
        "        image_path = self.drive_path + row[IMAGE_COL]\n",
        "        #open\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        #encode\n",
        "        image_encoding = self.image_processor(\n",
        "          images=image,\n",
        "          return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        #debug\n",
        "        if idx == 1:\n",
        "          print(f\"Image encoding shape for index {idx}: {image_encoding['pixel_values'].shape}\")\n",
        "\n",
        "        #pixel encode\n",
        "        pixel_values = image_encoding['pixel_values'].squeeze(0)\n",
        "\n",
        "        #debug\n",
        "        if idx == 1:\n",
        "          print(f\"Pixel values shape for index {idx}: {pixel_values.shape}\")\n",
        "\n",
        "      #never gets here - used for debug prev\n",
        "      except Exception as e:\n",
        "        print(f\"Error loading image at index {idx} ({image_path}): {e}\")\n",
        "        continue\n",
        "\n",
        "      #get label\n",
        "      label_value = row[LABEL_COL]\n",
        "      labels = None\n",
        "      try:\n",
        "        #label tensor\n",
        "        labels = torch.tensor(int(label_value), dtype=torch.long)\n",
        "\n",
        "      #never gets here - used for debug prev\n",
        "      except (ValueError, TypeError) as e:\n",
        "        print(f\"Error processing label for index {idx}: {e}, raw value: {label_value}\")\n",
        "        continue\n",
        "\n",
        "      #create item and add to processed_data\n",
        "      item = {\n",
        "        'input_ids': text_encoding['input_ids'].flatten(),\n",
        "        'attention_mask': text_encoding['attention_mask'].flatten(),\n",
        "        'pixel_values': pixel_values,\n",
        "        'labels': labels\n",
        "      }\n",
        "      processed_data.append(item)\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx]"
      ],
      "metadata": {
        "id": "gk7mmchQdwyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#init tokenizers\n",
        "text_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "image_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "MAX_LEN = 128\n",
        "\n",
        "#create datasets\n",
        "train_dataset = MultimodalDataset(train_df, text_tokenizer, image_processor, MAX_LEN, drive_path=\"/content/drive/MyDrive/\")\n",
        "val_dataset = MultimodalDataset(val_df, text_tokenizer, image_processor, MAX_LEN, drive_path=\"/content/drive/MyDrive/\")\n",
        "test_dataset = MultimodalDataset(test_df, text_tokenizer, image_processor, MAX_LEN, drive_path=\"/content/drive/MyDrive/\")\n",
        "\n",
        "print(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "1A9hpuir5zSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, CLIPModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Late Fusion version\n",
        "class MultimodalBertClipClassifier(nn.Module):\n",
        "\n",
        "  #dropout_rate=0.1, fusion_dim=512\n",
        "  def __init__(self, num_labels=2, dropout_rate=0.2, fusion_dim=1024, bert_model_path=None, clip_model_path=None):\n",
        "    super().__init__()\n",
        "\n",
        "    #load pretrained models - base model as fall bakc\n",
        "    if bert_model_path:\n",
        "      print(f\"Loading BERT model from {bert_model_path}\")\n",
        "      self.bert = BertModel.from_pretrained(bert_model_path)\n",
        "    else:\n",
        "      print(\"Loading base BERT model\")\n",
        "      self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    if clip_model_path:\n",
        "      print(f\"Loading CLIP model from {clip_model_path}\")\n",
        "      if str(clip_model_path).endswith(\".pt\"):\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        loaded_state_dict = torch.load(clip_model_path, map_location=device)\n",
        "\n",
        "        processed_clip_state_dict = {}\n",
        "        if \"model\" in loaded_state_dict and \"classifier\" in loaded_state_dict and isinstance(loaded_state_dict[\"model\"], dict):\n",
        "          print(f\"Attempting to extract CLIP weights from 'model' key in {clip_model_path}\")\n",
        "          for k, v in loaded_state_dict[\"model\"].items():\n",
        "              if k.startswith(\"model.\"):\n",
        "                processed_clip_state_dict[k.replace(\"model.\", \"\")] = v\n",
        "              else:\n",
        "                processed_clip_state_dict[k] = v\n",
        "        elif any(k.startswith(\"model.\") for k in loaded_state_dict.keys()):\n",
        "          print(f\"Stripping 'model.' prefix from keys in {clip_model_path}\")\n",
        "          for k, v in loaded_state_dict.items():\n",
        "            processed_clip_state_dict[k.replace(\"model.\", \"\")] = v\n",
        "        else:\n",
        "          processed_clip_state_dict = loaded_state_dict\n",
        "\n",
        "        self.clip.load_state_dict(processed_clip_state_dict, strict=False)\n",
        "        print(f\"Loaded CLIP state_dict from {clip_model_path} with strict=False\")\n",
        "      else:\n",
        "        self.clip = CLIPModel.from_pretrained(clip_model_path)\n",
        "    else:\n",
        "      print(\"Loading base CLIP model\")\n",
        "      self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "\n",
        "    #get embedding dims should be 768 for BERT\n",
        "    text_dim = self.bert.config.hidden_size\n",
        "    print(f\"Text embedding dimension: {text_dim}\")\n",
        "\n",
        "    #get embedding dims for clip\n",
        "    image_dim = self.clip.config.vision_config.projection_dim\n",
        "    print(f\"Image embedding dimension: {image_dim}\")\n",
        "\n",
        "    #late fusion\n",
        "    self.text_classifier = nn.Sequential(\n",
        "      nn.Linear(text_dim, fusion_dim // 2),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(fusion_dim // 2, num_labels)\n",
        "    )\n",
        "\n",
        "    self.image_classifier = nn.Sequential(\n",
        "      nn.Linear(image_dim, fusion_dim // 2),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(fusion_dim // 2, num_labels)\n",
        "    )\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n",
        "    #text embeddings\n",
        "    text_outputs = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "\n",
        "    #image embeddings\n",
        "    image_outputs = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "    #separate classifiers\n",
        "    text_logits = self.text_classifier(text_embeddings)\n",
        "    image_logits = self.image_classifier(image_outputs)\n",
        "\n",
        "    #late fusion\n",
        "    logits = text_logits + image_logits\n",
        "\n",
        "    return (logits,)\n",
        "\n",
        "#initialize\n",
        "model = MultimodalBertClipClassifier(\n",
        "  num_labels=2,\n",
        "  bert_model_path=\"/content/drive/MyDrive/data/models/fine_tuned_bert\",\n",
        "  clip_model_path=\"/content/drive/MyDrive/data/models/clip_lora_best.pt\"\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "cjTvKC-jd2-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, CLIPModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Early Fusion Version\n",
        "class MultimodalBertClipClassifier(nn.Module):\n",
        "  #dropout_rate=0.1, fusion_dim=512\n",
        "  def __init__(self, num_labels=2, dropout_rate=0.2, fusion_dim=1024, bert_model_path=None, clip_model_path=None):\n",
        "    super().__init__()\n",
        "\n",
        "    #load pretrained models - base model as fall bakc\n",
        "    if bert_model_path:\n",
        "      print(f\"Loading BERT model from {bert_model_path}\")\n",
        "      self.bert = BertModel.from_pretrained(bert_model_path)\n",
        "    else:\n",
        "      print(\"Loading base BERT model\")\n",
        "      self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    if clip_model_path:\n",
        "      print(f\"Loading CLIP model from {clip_model_path}\")\n",
        "      if str(clip_model_path).endswith(\".pt\"):\n",
        "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        loaded_state_dict = torch.load(clip_model_path, map_location=device)\n",
        "        processed_clip_state_dict = {}\n",
        "        if \"model\" in loaded_state_dict and \"classifier\" in loaded_state_dict and isinstance(loaded_state_dict[\"model\"], dict):\n",
        "          print(f\"Attempting to extract CLIP weights from 'model' key in {clip_model_path}\")\n",
        "          for k, v in loaded_state_dict[\"model\"].items():\n",
        "            if k.startswith(\"model.\"):\n",
        "              processed_clip_state_dict[k.replace(\"model.\", \"\")] = v\n",
        "            else:\n",
        "              processed_clip_state_dict[k] = v\n",
        "        elif any(k.startswith(\"model.\") for k in loaded_state_dict.keys()):\n",
        "          print(f\"Stripping 'model.' prefix from keys in {clip_model_path}\")\n",
        "          for k, v in loaded_state_dict.items():\n",
        "            processed_clip_state_dict[k.replace(\"model.\", \"\")] = v\n",
        "        else:\n",
        "          processed_clip_state_dict = loaded_state_dict\n",
        "\n",
        "        self.clip.load_state_dict(processed_clip_state_dict, strict=False)\n",
        "        print(f\"Loaded CLIP state_dict from {clip_model_path} with strict=False\")\n",
        "      else:\n",
        "        self.clip = CLIPModel.from_pretrained(clip_model_path)\n",
        "    else:\n",
        "      print(\"Loading base CLIP model\")\n",
        "      self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "\n",
        "    #get embedding dims = 768\n",
        "    text_dim = self.bert.config.hidden_size\n",
        "    print(f\"Text embedding dimension: {text_dim}\")\n",
        "    #image dims\n",
        "    image_dim = self.clip.config.vision_config.projection_dim\n",
        "    print(f\"Image embedding dimension: {image_dim}\")\n",
        "\n",
        "    #early fusion\n",
        "    self.fusion_classifier = nn.Sequential(\n",
        "      nn.Linear(text_dim + image_dim, fusion_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(fusion_dim, fusion_dim // 2),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout_rate),\n",
        "      nn.Linear(fusion_dim // 2, num_labels)\n",
        "    )\n",
        "\n",
        "  def forward(self, input_ids, attention_mask, pixel_values, labels=None): # Add labels=None to the forward signature\n",
        "    #text embeddings\n",
        "    text_outputs = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    text_embeddings = text_outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "\n",
        "    #image embddings\n",
        "    image_outputs = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "    #debug\n",
        "    #print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
        "    #print(f\"Image embeddings shape: {image_outputs.shape}\")\n",
        "\n",
        "    #early fusion\n",
        "    fused_embeddings = torch.cat([text_embeddings, image_outputs], dim=1)\n",
        "\n",
        "    #print(f\"Fused embeddings shape: {fused_embeddings.shape}\")\n",
        "\n",
        "    #classify\n",
        "    logits = self.fusion_classifier(fused_embeddings)\n",
        "\n",
        "    return (logits,)\n",
        "\n",
        "\n",
        "#initialize\n",
        "model = MultimodalBertClipClassifier(\n",
        "  num_labels=2,\n",
        "  bert_model_path=\"/content/drive/MyDrive/data/models/fine_tuned_bert\",\n",
        "  clip_model_path=\"/content/drive/MyDrive/data/models/clip_lora_best.pt\"\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "bt_LOOfAlTLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class MultimodalDataCollator:\n",
        "  def __init__(self, return_tensors=\"pt\"):\n",
        "    self.return_tensors = return_tensors\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    #batch size check debug\n",
        "    #print(f\"Processing batch of size: {len(batch)}\")\n",
        "\n",
        "    input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n",
        "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "\n",
        "    batch_out = {\n",
        "      'input_ids': input_ids,\n",
        "      'attention_mask': attention_mask,\n",
        "      'pixel_values': pixel_values\n",
        "    }\n",
        "\n",
        "    if 'labels' in batch[0]:\n",
        "      batch_out['labels'] = torch.tensor([item['labels'] for item in batch])\n",
        "    elif '2_way_label' in batch[0]:\n",
        "      batch_out['labels'] = torch.tensor([item['2_way_label'] for item in batch])\n",
        "    else:\n",
        "      print(\"No labels in this batch â€” likely evaluation/prediction batch.\")\n",
        "\n",
        "    return batch_out\n",
        "data_collator = MultimodalDataCollator(return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "5-EwIYWed9kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "import torch\n",
        "\n",
        "\n",
        "#compute class weights\n",
        "classes = np.array(sorted(train_df[LABEL_COL].unique()))\n",
        "weights = compute_class_weight(\n",
        "  class_weight=\"balanced\",\n",
        "  classes=classes,\n",
        "  y=train_df[LABEL_COL].values\n",
        ")\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "print(\"Class weights:\", class_weights, \"for classes:\", classes)\n",
        "\n",
        "class MultimodalTrainer(Trainer):\n",
        "  def __init__(self, class_weights=None, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.class_weights = class_weights\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    labels = inputs.pop(\"labels\")\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    #get logits\n",
        "    logits = outputs[0]\n",
        "\n",
        "    #handle smaller bathc sizes\n",
        "    if logits.size(0) != labels.size(0):\n",
        "      min_size = min(logits.size(0), labels.size(0))\n",
        "      logits = logits[:min_size]\n",
        "      labels = labels[:min_size]\n",
        "\n",
        "    if self.class_weights is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "    else:\n",
        "      loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss = loss_fct(logits, labels)\n",
        "    return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "  def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "    labels = inputs.pop(\"labels\") if \"labels\" in inputs else None\n",
        "\n",
        "    with torch.no_grad():\n",
        "      #forward pass\n",
        "      outputs = model(**inputs)\n",
        "\n",
        "      #get logits\n",
        "      if isinstance(outputs, tuple):\n",
        "        logits = outputs[0]\n",
        "      else:\n",
        "        logits = outputs\n",
        "\n",
        "      #compute loss\n",
        "      if labels is not None:\n",
        "        if self.class_weights is not None:\n",
        "          loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
        "        else:\n",
        "          loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(logits, labels)\n",
        "      else:\n",
        "        loss = None\n",
        "\n",
        "    #return proper format\n",
        "    if prediction_loss_only:\n",
        "      return (loss, None, None)\n",
        "\n",
        "    return (loss, logits, labels)\n",
        "\n",
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "  #verfy eval_pred\n",
        "  if not isinstance(eval_pred, EvalPrediction) or not hasattr(eval_pred, 'predictions') or eval_pred.predictions is None:\n",
        "    print(f\"Warning: compute_metrics received invalid or empty EvalPrediction object.\")\n",
        "    return {\"accuracy\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "  logits = eval_pred.predictions\n",
        "  labels = eval_pred.label_ids\n",
        "\n",
        "  #verify i get tuple\n",
        "  if isinstance(logits, tuple):\n",
        "    if len(logits) > 0:\n",
        "      logits = logits[0]\n",
        "    else:\n",
        "      print(\"Warning: compute_metrics received empty logits tuple from EvalPrediction.\")\n",
        "      return {\"accuracy\": 0.0, \"f1\": 0.0}\n",
        "\n",
        "  #convert to numpy if needed\n",
        "  if isinstance(logits, torch.Tensor):\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "  if isinstance(labels, torch.Tensor):\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "  #predict\n",
        "  preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "  return {\n",
        "    \"accuracy\": float(accuracy_score(labels, preds)),\n",
        "    \"f1\": float(f1_score(labels, preds, average=\"weighted\")),\n",
        "  }\n",
        "\n",
        "\n",
        "\n",
        "per_bs = 8 if (use_mps and not use_cuda) else 16\n",
        "grad_acc = 4 if (use_mps and not use_cuda) else 2\n",
        "\n",
        "#training args\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=f\"{OUT_DIR}/multimodal_bert_clip\",\n",
        "  eval_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  metric_for_best_model=\"f1\",\n",
        "  greater_is_better=True,\n",
        "\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=100,\n",
        "  per_device_train_batch_size=per_bs,\n",
        "  per_device_eval_batch_size=max(16, per_bs),\n",
        "  gradient_accumulation_steps=grad_acc,\n",
        "\n",
        "  num_train_epochs=8,\n",
        "  learning_rate=5e-6,\n",
        "  weight_decay=0.02,\n",
        "  warmup_ratio=0.10,\n",
        "  lr_scheduler_type=\"cosine\",\n",
        "\n",
        "  fp16=use_cuda,\n",
        "  bf16=False,\n",
        "  dataloader_num_workers=2,\n",
        "  report_to=\"none\",\n",
        "  seed=SEED,\n",
        "  eval_accumulation_steps=4,\n",
        ")\n",
        "\n",
        "\n",
        "#create trainer\n",
        "trainer = MultimodalTrainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=val_dataset,\n",
        "  data_collator=data_collator,\n",
        "  compute_metrics=compute_metrics,\n",
        "  class_weights=class_weights,\n",
        "  callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
        ")\n",
        "\n",
        "\n",
        "#dataset size - debug\n",
        "print(f\"Size of validation dataset before training: {len(val_dataset)}\")\n",
        "\n",
        "\n",
        "#train model\n",
        "print(\"Starting multimodal fine-tuning...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "\n",
        "#eval validation set\n",
        "print(f\"Size of validation dataset before evaluation: {len(val_dataset)}\")\n",
        "val_metrics = trainer.evaluate(eval_dataset=val_dataset)\n",
        "print(\"Validation metrics:\", val_metrics)\n"
      ],
      "metadata": {
        "id": "6uw5oybFUGLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#test set eval\n",
        "print(\"Starting test evaluation...\")\n",
        "test_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(\"Test metrics:\", test_metrics)\n",
        "\n",
        "'''\n",
        "output_model_dir = f\"{OUT_DIR}/final_model\"\n",
        "Path(output_model_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), f\"{output_model_dir}/pytorch_model.bin\")\n",
        "\n",
        "text_tokenizer.save_pretrained(output_model_dir)\n",
        "image_processor.save_pretrained(output_model_dir)\n",
        "'''\n",
        "\n",
        "print(\"Multimodal fine-tuning and saving done\")"
      ],
      "metadata": {
        "id": "AXtZzzhieEN8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}